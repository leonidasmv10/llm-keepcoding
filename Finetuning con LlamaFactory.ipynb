{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAE-CXd8rYW1"
   },
   "source": [
    "# Descargamos las dependencias y el repositorio de LlamaFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90173,
     "status": "ok",
     "timestamp": 1749406023837,
     "user": {
      "displayName": "Leonidas Moreno Vásquez",
      "userId": "09981213990272163392"
     },
     "user_tz": -120
    },
    "id": "R6rkLYOykFDM",
    "outputId": "da5b5abc-e08d-408e-8a00-e529b3ca53a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\n",
      "remote: Enumerating objects: 360, done.\u001b[K\n",
      "remote: Counting objects: 100% (360/360), done.\u001b[K\n",
      "remote: Compressing objects: 100% (276/276), done.\u001b[K\n",
      "remote: Total 360 (delta 80), reused 272 (delta 69), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (360/360), 9.93 MiB | 11.04 MiB/s, done.\n",
      "Resolving deltas: 100% (80/80), done.\n",
      "/content/LLaMA-Factory\n",
      "Obtaining file:///content/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (4.52.4)\n",
      "Collecting datasets<=3.6.0,>=2.16.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: accelerate<=1.7.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.7.0)\n",
      "Requirement already satisfied: peft<=0.15.2,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.15.2)\n",
      "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: tokenizers<=0.21.1,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.21.1)\n",
      "Requirement already satisfied: gradio<=5.31.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (5.31.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.15.3)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.9.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (5.29.5)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.34.3)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.115.12)\n",
      "Collecting sse-starlette (from llamafactory==0.9.3.dev0)\n",
      "  Downloading sse_starlette-2.3.6-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.10.0)\n",
      "Collecting fire (from llamafactory==0.9.3.dev0)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
      "Collecting numpy<2.0.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<=2.10.6 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.2)\n",
      "Collecting av (from llamafactory==0.9.3.dev0)\n",
      "  Downloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.11.0)\n",
      "Collecting tyro<0.9.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.21.0+cu124)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.9.1)\n",
      "Requirement already satisfied: jieba in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.42.1)\n",
      "Collecting rouge-chinese (from llamafactory==0.9.3.dev0)\n",
      "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3.dev0) (5.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.70.15)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.9.0)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.6.0)\n",
      "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.10.1)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.18)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.2.1)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.11.12)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.14.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (0.7.0)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<=2.10.6->llamafactory==0.9.3.dev0)\n",
      "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (3.5)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->llamafactory==0.9.3.dev0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0->llamafactory==0.9.3.dev0) (2024.11.6)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.9.4)\n",
      "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.2.1)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.16.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.3.dev0) (3.1.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.60.0)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.5.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (4.4.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->llamafactory==0.9.3.dev0) (4.9.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge-chinese->llamafactory==0.9.3.dev0) (1.17.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.15)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.9)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3.dev0) (1.1.2)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.19.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.3.dev0) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.20.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Downloading sse_starlette-2.3.6-py3-none-any.whl (10 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: llamafactory, fire\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=27420 sha256=56fa01edafac82dbf92e5bf0f26e7c5e019a059c65313ed1e2d5ce7066953b8c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-k3yq9f2r/wheels/bd/34/05/1e3cb4b8f20c20631b411dc5157b4b150850c03496fa96c2c4\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=60fa069486d4673e7d97a4fb5c9cd412bb928f6bb378a9f80a92189ab607163d\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
      "Successfully built llamafactory fire\n",
      "Installing collected packages: shtab, rouge-chinese, pydantic-core, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, fsspec, fire, av, sse-starlette, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tyro, nvidia-cusolver-cu12, datasets, trl, llamafactory\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.33.2\n",
      "    Uninstalling pydantic_core-2.33.2:\n",
      "      Successfully uninstalled pydantic_core-2.33.2\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.11.5\n",
      "    Uninstalling pydantic-2.11.5:\n",
      "      Successfully uninstalled pydantic-2.11.5\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.4\n",
      "    Uninstalling datasets-2.14.4:\n",
      "      Successfully uninstalled datasets-2.14.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed av-14.4.0 datasets-3.6.0 fire-0.7.0 fsspec-2025.3.0 llamafactory-0.9.3.dev0 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydantic-2.10.6 pydantic-core-2.27.2 rouge-chinese-1.0.3 shtab-1.7.2 sse-starlette-2.3.6 trl-0.9.6 tyro-0.8.14\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "%cd LLaMA-Factory\n",
    "!pip install -e \".[torch,metrics]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1710832,
     "status": "ok",
     "timestamp": 1749407789279,
     "user": {
      "displayName": "Leonidas Moreno Vásquez",
      "userId": "09981213990272163392"
     },
     "user_tz": -120
    },
    "id": "cHHuIZR5kKMM",
    "outputId": "38d80229-351d-4822-84f9-3eab01f57102"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-08 18:08:11.362012: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-08 18:08:11.378099: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749406091.397443    4127 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749406091.404032    4127 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-08 18:08:11.425962: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* Running on public URL: https://8f5f3dd56cb43d78f0.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
      "2025-06-08 18:11:10.408061: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749406270.429768    4967 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749406270.436487    4967 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[INFO|2025-06-08 18:11:17] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
      "tokenizer_config.json: 100% 7.30k/7.30k [00:00<00:00, 35.9MB/s]\n",
      "vocab.json: 100% 2.78M/2.78M [00:01<00:00, 2.56MB/s]\n",
      "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 1.91MB/s]\n",
      "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 10.6MB/s]\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:11:23,163 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:11:23,163 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:11:23,163 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:11:23,163 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:11:23,164 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:11:23,164 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:11:23,164 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2299] 2025-06-08 18:11:23,553 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "config.json: 100% 661/661 [00:00<00:00, 5.34MB/s]\n",
      "[INFO|configuration_utils.py:698] 2025-06-08 18:11:25,208 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-06-08 18:11:25,216 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 70,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:11:25,680 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:11:25,680 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:11:25,680 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:11:25,680 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:11:25,681 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:11:25,681 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:11:25,681 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2299] 2025-06-08 18:11:26,044 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-06-08 18:11:26] llamafactory.data.loader:143 >> Loading dataset es-prehistoric-animals-tutor-dataset.json...\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "WARNING:datasets.builder:Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 1010 examples [00:00, 23008.95 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100% 1010/1010 [00:00<00:00, 4166.86 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16): 100% 1010/1010 [00:02<00:00, 337.22 examples/s]\n",
      "training example:\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 30182, 65706, 1531, 650, 49495, 1020, 436, 42513, 41807, 30, 151645, 198, 151644, 77091, 198, 39832, 68012, 521, 21761, 0, 3984, 49495, 1020, 436, 42513, 41807, 11385, 7953, 655, 5517, 3165, 1594, 3619, 10383, 11, 19438, 409, 34805, 220, 21, 20, 63024, 409, 26748, 11, 28286, 77729, 30, 76433, 24217, 349, 650, 98999, 33762, 409, 8750, 62314, 436, 481, 68510, 36460, 409, 23299, 11385, 10351, 293, 77115, 13, 16288, 294, 26240, 2714, 276, 7953, 625, 16685, 43605, 22583, 15477, 11, 19438, 7977, 321, 5553, 7953, 272, 1387, 64937, 13, 48813, 65706, 53390, 5845, 0, 47588, 655, 312, 88, 409, 2478, 62889, 3530, 11, 259, 41725, 13, 151645, 198]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "¿Qué es un Tyrannosaurus Rex?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "¡Hola chaval! El Tyrannosaurus Rex era como el matón del barrio, pero de hace 65 millones de años, ¿vale? Imagínate un autobús de dos pisos - pues así de alto era este bicho. Sus dientes eran como plátanos gigantes, pero afilados como cuchillos. ¡Qué barbaridad! Era el rey de los dinosaurios, tío.<|im_end|>\n",
      "\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 39832, 68012, 521, 21761, 0, 3984, 49495, 1020, 436, 42513, 41807, 11385, 7953, 655, 5517, 3165, 1594, 3619, 10383, 11, 19438, 409, 34805, 220, 21, 20, 63024, 409, 26748, 11, 28286, 77729, 30, 76433, 24217, 349, 650, 98999, 33762, 409, 8750, 62314, 436, 481, 68510, 36460, 409, 23299, 11385, 10351, 293, 77115, 13, 16288, 294, 26240, 2714, 276, 7953, 625, 16685, 43605, 22583, 15477, 11, 19438, 7977, 321, 5553, 7953, 272, 1387, 64937, 13, 48813, 65706, 53390, 5845, 0, 47588, 655, 312, 88, 409, 2478, 62889, 3530, 11, 259, 41725, 13, 151645, 198]\n",
      "labels:\n",
      "¡Hola chaval! El Tyrannosaurus Rex era como el matón del barrio, pero de hace 65 millones de años, ¿vale? Imagínate un autobús de dos pisos - pues así de alto era este bicho. Sus dientes eran como plátanos gigantes, pero afilados como cuchillos. ¡Qué barbaridad! Era el rey de los dinosaurios, tío.<|im_end|>\n",
      "\n",
      "[INFO|configuration_utils.py:698] 2025-06-08 18:11:31,038 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-06-08 18:11:31,039 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 70,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|2025-06-08 18:11:31] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "model.safetensors.index.json: 100% 35.6k/35.6k [00:00<00:00, 127MB/s]\n",
      "[INFO|modeling_utils.py:1151] 2025-06-08 18:11:32,468 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/model.safetensors.index.json\n",
      "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0% 0.00/3.97G [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   0% 0.00/2.20G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   0% 20.3k/3.97G [00:01<88:08:56, 12.5kB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0% 19.1M/3.97G [00:01<04:37, 14.2MB/s]   \u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   0% 898k/2.20G [00:01<1:16:10, 482kB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   1% 44.2M/3.97G [00:01<01:45, 37.0MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   0% 9.28M/2.20G [00:02<05:53, 6.20MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   2% 76.5M/3.97G [00:02<00:55, 69.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3% 119M/3.97G [00:02<00:32, 118MB/s]  \u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   3% 66.4M/2.20G [00:02<00:41, 51.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   5% 186M/3.97G [00:02<00:18, 210MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8% 314M/3.97G [00:02<00:08, 417MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   6% 128M/2.20G [00:02<00:20, 103MB/s]  \u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   8% 173M/2.20G [00:02<00:14, 136MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  11% 448M/3.97G [00:02<00:09, 390MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   9% 199M/2.20G [00:02<00:15, 128MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  14% 566M/3.97G [00:02<00:07, 463MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  10% 221M/2.20G [00:03<00:30, 64.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  17% 663M/3.97G [00:03<00:15, 212MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20% 778M/3.97G [00:04<00:13, 233MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  11% 239M/2.20G [00:04<00:39, 49.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  22% 855M/3.97G [00:06<00:31, 97.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23% 930M/3.97G [00:07<00:30, 101MB/s] \u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  14% 311M/2.20G [00:07<00:57, 32.7MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  24% 962M/3.97G [00:07<00:27, 108MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  18% 394M/2.20G [00:07<00:31, 58.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  25% 1.01G/3.97G [00:07<00:23, 127MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  24% 524M/2.20G [00:07<00:14, 112MB/s] \u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  27% 1.07G/3.97G [00:07<00:17, 165MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  27% 591M/2.20G [00:07<00:11, 140MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  28% 1.11G/3.97G [00:07<00:18, 157MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  30% 662M/2.20G [00:07<00:08, 176MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  29% 1.15G/3.97G [00:08<00:19, 148MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  34% 740M/2.20G [00:08<00:09, 154MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  30% 1.18G/3.97G [00:08<00:22, 122MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  35% 780M/2.20G [00:11<00:26, 54.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  38% 847M/2.20G [00:11<00:18, 74.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  32% 1.26G/3.97G [00:11<00:55, 48.8MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  41% 903M/2.20G [00:11<00:13, 95.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  34% 1.33G/3.97G [00:11<00:37, 70.2MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  46% 1.01G/2.20G [00:11<00:07, 150MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  36% 1.42G/3.97G [00:11<00:23, 108MB/s] \u001b[A\n",
      "model-00001-of-00002.safetensors:  37% 1.48G/3.97G [00:12<00:25, 98.5MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  49% 1.07G/2.20G [00:12<00:09, 117MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  51% 1.12G/2.20G [00:13<00:10, 98.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  40% 1.59G/3.97G [00:13<00:22, 107MB/s] \u001b[A\n",
      "model-00001-of-00002.safetensors:  41% 1.62G/3.97G [00:13<00:19, 122MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  53% 1.16G/2.20G [00:14<00:12, 84.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  55% 1.20G/2.20G [00:14<00:09, 101MB/s] \u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  43% 1.71G/3.97G [00:14<00:19, 116MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45% 1.78G/3.97G [00:14<00:15, 141MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  58% 1.28G/2.20G [00:15<00:10, 91.2MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  47% 1.85G/3.97G [00:15<00:17, 118MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  59% 1.30G/2.20G [00:15<00:11, 78.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  48% 1.91G/3.97G [00:16<00:18, 114MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50% 1.98G/3.97G [00:16<00:13, 142MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  60% 1.32G/2.20G [00:16<00:13, 63.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  51% 2.01G/3.97G [00:16<00:12, 152MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  62% 1.37G/2.20G [00:16<00:11, 70.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  65% 1.43G/2.20G [00:19<00:18, 41.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  51% 2.04G/3.97G [00:19<00:47, 40.4MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  69% 1.51G/2.20G [00:19<00:10, 65.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  72% 1.58G/2.20G [00:19<00:06, 93.7MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  53% 2.09G/3.97G [00:19<00:36, 50.9MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  74% 1.63G/2.20G [00:20<00:05, 105MB/s] \u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  55% 2.18G/3.97G [00:20<00:21, 81.6MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  77% 1.70G/2.20G [00:20<00:03, 143MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  57% 2.25G/3.97G [00:20<00:15, 110MB/s] \u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  79% 1.74G/2.20G [00:20<00:02, 159MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  59% 2.33G/3.97G [00:20<00:10, 158MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  81% 1.78G/2.20G [00:20<00:02, 153MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  83% 1.82G/2.20G [00:21<00:04, 91.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  85% 1.88G/2.20G [00:25<00:09, 32.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  60% 2.38G/3.97G [00:25<00:46, 34.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62% 2.45G/3.97G [00:25<00:32, 47.4MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  89% 1.95G/2.20G [00:25<00:05, 49.1MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  63% 2.52G/3.97G [00:25<00:21, 66.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66% 2.60G/3.97G [00:26<00:14, 93.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67% 2.67G/3.97G [00:26<00:10, 121MB/s] \u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  91% 2.02G/2.20G [00:26<00:03, 57.9MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:  68% 2.71G/3.97G [00:26<00:09, 128MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  95% 2.08G/2.20G [00:26<00:01, 79.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  97% 2.14G/2.20G [00:26<00:00, 101MB/s] \u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors: 100% 2.20G/2.20G [00:27<00:00, 81.0MB/s]\n",
      "\n",
      "model-00001-of-00002.safetensors:  70% 2.78G/3.97G [00:27<00:09, 124MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71% 2.83G/3.97G [00:27<00:07, 145MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73% 2.88G/3.97G [00:27<00:06, 175MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74% 2.92G/3.97G [00:27<00:05, 182MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76% 3.01G/3.97G [00:28<00:04, 192MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77% 3.05G/3.97G [00:28<00:05, 176MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78% 3.08G/3.97G [00:29<00:07, 114MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79% 3.14G/3.97G [00:29<00:05, 150MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81% 3.21G/3.97G [00:29<00:03, 193MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82% 3.27G/3.97G [00:29<00:04, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84% 3.33G/3.97G [00:30<00:03, 206MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85% 3.39G/3.97G [00:30<00:02, 235MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87% 3.46G/3.97G [00:30<00:01, 274MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89% 3.52G/3.97G [00:30<00:01, 268MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90% 3.59G/3.97G [00:30<00:01, 300MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92% 3.64G/3.97G [00:30<00:01, 320MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93% 3.68G/3.97G [00:31<00:01, 240MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94% 3.72G/3.97G [00:31<00:01, 214MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95% 3.77G/3.97G [00:32<00:01, 134MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97% 3.84G/3.97G [00:32<00:00, 182MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98% 3.91G/3.97G [00:32<00:00, 206MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100% 3.97G/3.97G [00:32<00:00, 121MB/s]\n",
      "Fetching 2 files: 100% 2/2 [00:33<00:00, 16.65s/it]\n",
      "[INFO|modeling_utils.py:2241] 2025-06-08 18:12:06,016 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1135] 2025-06-08 18:12:06,018 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.04s/it]\n",
      "[INFO|modeling_utils.py:5131] 2025-06-08 18:12:08,158 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:5139] 2025-06-08 18:12:08,158 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-3B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "generation_config.json: 100% 242/242 [00:00<00:00, 1.98MB/s]\n",
      "[INFO|configuration_utils.py:1090] 2025-06-08 18:12:08,627 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/generation_config.json\n",
      "[INFO|configuration_utils.py:1135] 2025-06-08 18:12:08,627 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "[INFO|2025-06-08 18:12:08] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-06-08 18:12:08] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-06-08 18:12:08] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-06-08 18:12:08] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-06-08 18:12:08] llamafactory.model.model_utils.misc:143 >> Found linear modules: q_proj,o_proj,gate_proj,down_proj,up_proj,k_proj,v_proj\n",
      "[INFO|2025-06-08 18:12:09] llamafactory.model.loader:143 >> trainable params: 14,966,784 || all params: 3,100,905,472 || trainable%: 0.4827\n",
      "[INFO|trainer.py:756] 2025-06-08 18:12:09,399 >> Using auto half precision backend\n",
      "[INFO|2025-06-08 18:12:09] llamafactory.train.trainer_utils:143 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
      "[INFO|trainer.py:2409] 2025-06-08 18:12:09,909 >> ***** Running training *****\n",
      "[INFO|trainer.py:2410] 2025-06-08 18:12:09,910 >>   Num examples = 1,010\n",
      "[INFO|trainer.py:2411] 2025-06-08 18:12:09,910 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2412] 2025-06-08 18:12:09,910 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2415] 2025-06-08 18:12:09,910 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2416] 2025-06-08 18:12:09,910 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2417] 2025-06-08 18:12:09,910 >>   Total optimization steps = 192\n",
      "[INFO|trainer.py:2418] 2025-06-08 18:12:09,915 >>   Number of trainable parameters = 14,966,784\n",
      "  3% 5/192 [00:18<11:10,  3.58s/it][INFO|2025-06-08 18:12:28] llamafactory.train.callbacks:143 >> {'loss': 1.7783, 'learning_rate': 4.9946e-05, 'epoch': 0.08, 'throughput': 679.21}\n",
      "{'loss': 1.7783, 'grad_norm': 0.6165962815284729, 'learning_rate': 4.994647308096509e-05, 'epoch': 0.08, 'num_input_tokens_seen': 12896, 'train_runtime': 18.992, 'train_tokens_per_second': 679.022}\n",
      "  5% 10/192 [00:36<10:29,  3.46s/it][INFO|2025-06-08 18:12:46] llamafactory.train.callbacks:143 >> {'loss': 1.3361, 'learning_rate': 4.9729e-05, 'epoch': 0.16, 'throughput': 724.48}\n",
      "{'loss': 1.3361, 'grad_norm': 0.45620644092559814, 'learning_rate': 4.972941274911953e-05, 'epoch': 0.16, 'num_input_tokens_seen': 26224, 'train_runtime': 36.2024, 'train_tokens_per_second': 724.373}\n",
      "  8% 15/192 [00:53<10:03,  3.41s/it][INFO|2025-06-08 18:13:03] llamafactory.train.callbacks:143 >> {'loss': 1.2288, 'learning_rate': 4.9347e-05, 'epoch': 0.24, 'throughput': 742.48}\n",
      "{'loss': 1.2288, 'grad_norm': 0.5080089569091797, 'learning_rate': 4.934692448193334e-05, 'epoch': 0.24, 'num_input_tokens_seen': 39504, 'train_runtime': 53.2107, 'train_tokens_per_second': 742.407}\n",
      " 10% 20/192 [01:10<09:44,  3.40s/it][INFO|2025-06-08 18:13:20] llamafactory.train.callbacks:143 >> {'loss': 1.2356, 'learning_rate': 4.8802e-05, 'epoch': 0.32, 'throughput': 740.60}\n",
      "{'loss': 1.2356, 'grad_norm': 0.5243834257125854, 'learning_rate': 4.880156694284811e-05, 'epoch': 0.32, 'num_input_tokens_seen': 52000, 'train_runtime': 70.2189, 'train_tokens_per_second': 740.541}\n",
      " 13% 25/192 [01:27<09:28,  3.40s/it][INFO|2025-06-08 18:13:37] llamafactory.train.callbacks:143 >> {'loss': 1.1074, 'learning_rate': 4.8097e-05, 'epoch': 0.40, 'throughput': 746.95}\n",
      "{'loss': 1.1074, 'grad_norm': 0.5924198627471924, 'learning_rate': 4.8096988312782174e-05, 'epoch': 0.4, 'num_input_tokens_seen': 65184, 'train_runtime': 87.2722, 'train_tokens_per_second': 746.905}\n",
      " 16% 30/192 [01:44<09:15,  3.43s/it][INFO|2025-06-08 18:13:54] llamafactory.train.callbacks:143 >> {'loss': 1.0370, 'learning_rate': 4.7238e-05, 'epoch': 0.48, 'throughput': 747.52}\n",
      "{'loss': 1.037, 'grad_norm': 0.47577452659606934, 'learning_rate': 4.7237901885546404e-05, 'epoch': 0.48, 'num_input_tokens_seen': 78048, 'train_runtime': 104.4146, 'train_tokens_per_second': 747.482}\n",
      " 18% 35/192 [02:01<08:54,  3.41s/it][INFO|2025-06-08 18:14:11] llamafactory.train.callbacks:143 >> {'loss': 0.9488, 'learning_rate': 4.6230e-05, 'epoch': 0.55, 'throughput': 752.75}\n",
      "{'loss': 0.9488, 'grad_norm': 0.6639350652694702, 'learning_rate': 4.6230054538164474e-05, 'epoch': 0.55, 'num_input_tokens_seen': 91424, 'train_runtime': 121.458, 'train_tokens_per_second': 752.721}\n",
      " 21% 40/192 [02:18<08:38,  3.41s/it][INFO|2025-06-08 18:14:28] llamafactory.train.callbacks:143 >> {'loss': 0.9211, 'learning_rate': 4.5080e-05, 'epoch': 0.63, 'throughput': 754.46}\n",
      "{'loss': 0.9211, 'grad_norm': 0.5488674640655518, 'learning_rate': 4.508018828701612e-05, 'epoch': 0.63, 'num_input_tokens_seen': 104496, 'train_runtime': 138.5089, 'train_tokens_per_second': 754.435}\n",
      " 23% 45/192 [02:35<08:20,  3.40s/it][INFO|2025-06-08 18:14:45] llamafactory.train.callbacks:143 >> {'loss': 0.9110, 'learning_rate': 4.3796e-05, 'epoch': 0.71, 'throughput': 753.65}\n",
      "{'loss': 0.911, 'grad_norm': 0.4937397241592407, 'learning_rate': 4.379599518697444e-05, 'epoch': 0.71, 'num_input_tokens_seen': 117184, 'train_runtime': 155.4944, 'train_tokens_per_second': 753.622}\n",
      " 26% 50/192 [02:52<08:07,  3.43s/it][INFO|2025-06-08 18:15:02] llamafactory.train.callbacks:143 >> {'loss': 0.7626, 'learning_rate': 4.2386e-05, 'epoch': 0.79, 'throughput': 754.55}\n",
      "{'loss': 0.7626, 'grad_norm': 0.4874994158744812, 'learning_rate': 4.238606587524029e-05, 'epoch': 0.79, 'num_input_tokens_seen': 130256, 'train_runtime': 172.6333, 'train_tokens_per_second': 754.524}\n",
      " 29% 55/192 [03:09<07:48,  3.42s/it][INFO|2025-06-08 18:15:19] llamafactory.train.callbacks:143 >> {'loss': 0.8315, 'learning_rate': 4.0860e-05, 'epoch': 0.87, 'throughput': 754.35}\n",
      "{'loss': 0.8315, 'grad_norm': 0.5382265448570251, 'learning_rate': 4.085983210409114e-05, 'epoch': 0.87, 'num_input_tokens_seen': 143104, 'train_runtime': 189.7094, 'train_tokens_per_second': 754.333}\n",
      " 31% 60/192 [03:26<07:33,  3.44s/it][INFO|2025-06-08 18:15:36] llamafactory.train.callbacks:143 >> {'loss': 0.7776, 'learning_rate': 3.9228e-05, 'epoch': 0.95, 'throughput': 754.50}\n",
      "{'loss': 0.7776, 'grad_norm': 0.5453691482543945, 'learning_rate': 3.922750364697246e-05, 'epoch': 0.95, 'num_input_tokens_seen': 156096, 'train_runtime': 206.8913, 'train_tokens_per_second': 754.483}\n",
      " 34% 65/192 [03:41<05:55,  2.80s/it][INFO|2025-06-08 18:15:50] llamafactory.train.callbacks:143 >> {'loss': 0.6382, 'learning_rate': 3.7500e-05, 'epoch': 1.02, 'throughput': 755.38}\n",
      "{'loss': 0.6382, 'grad_norm': 1.2203835248947144, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.02, 'num_input_tokens_seen': 166960, 'train_runtime': 221.034, 'train_tokens_per_second': 755.359}\n",
      " 36% 70/192 [03:58<06:43,  3.31s/it][INFO|2025-06-08 18:16:07] llamafactory.train.callbacks:143 >> {'loss': 0.5143, 'learning_rate': 3.5689e-05, 'epoch': 1.10, 'throughput': 757.43}\n",
      "{'loss': 0.5143, 'grad_norm': 0.7699535489082336, 'learning_rate': 3.568887733575706e-05, 'epoch': 1.1, 'num_input_tokens_seen': 180320, 'train_runtime': 238.0741, 'train_tokens_per_second': 757.411}\n",
      " 39% 75/192 [04:15<06:38,  3.40s/it][INFO|2025-06-08 18:16:25] llamafactory.train.callbacks:143 >> {'loss': 0.4912, 'learning_rate': 3.3806e-05, 'epoch': 1.17, 'throughput': 757.15}\n",
      "{'loss': 0.4912, 'grad_norm': 0.6079574823379517, 'learning_rate': 3.380625119803084e-05, 'epoch': 1.17, 'num_input_tokens_seen': 193216, 'train_runtime': 255.1949, 'train_tokens_per_second': 757.131}\n",
      " 42% 80/192 [04:32<06:22,  3.41s/it][INFO|2025-06-08 18:16:42] llamafactory.train.callbacks:143 >> {'loss': 0.5041, 'learning_rate': 3.1865e-05, 'epoch': 1.25, 'throughput': 757.38}\n",
      "{'loss': 0.5041, 'grad_norm': 0.8211813569068909, 'learning_rate': 3.186471545462331e-05, 'epoch': 1.25, 'num_input_tokens_seen': 206224, 'train_runtime': 272.2926, 'train_tokens_per_second': 757.362}\n",
      " 44% 85/192 [04:49<06:05,  3.41s/it][INFO|2025-06-08 18:16:59] llamafactory.train.callbacks:143 >> {'loss': 0.4963, 'learning_rate': 2.9877e-05, 'epoch': 1.33, 'throughput': 757.58}\n",
      "{'loss': 0.4963, 'grad_norm': 0.6266081929206848, 'learning_rate': 2.9877258050403212e-05, 'epoch': 1.33, 'num_input_tokens_seen': 219216, 'train_runtime': 289.3697, 'train_tokens_per_second': 757.564}\n",
      " 47% 90/192 [05:06<05:47,  3.41s/it][INFO|2025-06-08 18:17:16] llamafactory.train.callbacks:143 >> {'loss': 0.5066, 'learning_rate': 2.7857e-05, 'epoch': 1.41, 'throughput': 757.33}\n",
      "{'loss': 0.5066, 'grad_norm': 0.6034240126609802, 'learning_rate': 2.7857174124171165e-05, 'epoch': 1.41, 'num_input_tokens_seen': 232032, 'train_runtime': 306.386, 'train_tokens_per_second': 757.319}\n",
      " 49% 95/192 [05:23<05:31,  3.42s/it][INFO|2025-06-08 18:17:33] llamafactory.train.callbacks:143 >> {'loss': 0.4481, 'learning_rate': 2.5818e-05, 'epoch': 1.49, 'throughput': 757.33}\n",
      "{'loss': 0.4481, 'grad_norm': 0.7396795153617859, 'learning_rate': 2.5817977070544407e-05, 'epoch': 1.49, 'num_input_tokens_seen': 244976, 'train_runtime': 323.478, 'train_tokens_per_second': 757.319}\n",
      " 52% 100/192 [05:40<05:14,  3.42s/it][INFO|2025-06-08 18:17:50] llamafactory.train.callbacks:143 >> {'loss': 0.4349, 'learning_rate': 2.3773e-05, 'epoch': 1.57, 'throughput': 757.11}\n",
      "{'loss': 0.4349, 'grad_norm': 0.6180415749549866, 'learning_rate': 2.3773308141814552e-05, 'epoch': 1.57, 'num_input_tokens_seen': 257824, 'train_runtime': 340.5431, 'train_tokens_per_second': 757.096}\n",
      " 52% 100/192 [05:40<05:14,  3.42s/it][INFO|trainer.py:3993] 2025-06-08 18:17:50,459 >> Saving model checkpoint to saves/Qwen2.5-3B-Instruct/lora/train_2025-06-08-18-10-15/checkpoint-100\n",
      "[INFO|configuration_utils.py:698] 2025-06-08 18:17:50,960 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-06-08 18:17:50,961 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 70,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2356] 2025-06-08 18:17:51,103 >> chat template saved in saves/Qwen2.5-3B-Instruct/lora/train_2025-06-08-18-10-15/checkpoint-100/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2525] 2025-06-08 18:17:51,103 >> tokenizer config file saved in saves/Qwen2.5-3B-Instruct/lora/train_2025-06-08-18-10-15/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2534] 2025-06-08 18:17:51,104 >> Special tokens file saved in saves/Qwen2.5-3B-Instruct/lora/train_2025-06-08-18-10-15/checkpoint-100/special_tokens_map.json\n",
      " 55% 105/192 [05:58<05:04,  3.50s/it][INFO|2025-06-08 18:18:08] llamafactory.train.callbacks:143 >> {'loss': 0.3881, 'learning_rate': 2.1737e-05, 'epoch': 1.65, 'throughput': 755.08}\n",
      "{'loss': 0.3881, 'grad_norm': 0.699738621711731, 'learning_rate': 2.173684519449872e-05, 'epoch': 1.65, 'num_input_tokens_seen': 270864, 'train_runtime': 358.7261, 'train_tokens_per_second': 755.072}\n",
      " 57% 110/192 [06:15<04:44,  3.47s/it][INFO|2025-06-08 18:18:25] llamafactory.train.callbacks:143 >> {'loss': 0.4149, 'learning_rate': 1.9722e-05, 'epoch': 1.73, 'throughput': 754.64}\n",
      "{'loss': 0.4149, 'grad_norm': 0.644750714302063, 'learning_rate': 1.972221119102587e-05, 'epoch': 1.73, 'num_input_tokens_seen': 283728, 'train_runtime': 375.9833, 'train_tokens_per_second': 754.629}\n",
      " 60% 115/192 [06:33<04:25,  3.44s/it][INFO|2025-06-08 18:18:43] llamafactory.train.callbacks:143 >> {'loss': 0.4111, 'learning_rate': 1.7743e-05, 'epoch': 1.81, 'throughput': 754.05}\n",
      "{'loss': 0.4111, 'grad_norm': 0.8082276582717896, 'learning_rate': 1.7742883068638447e-05, 'epoch': 1.81, 'num_input_tokens_seen': 296496, 'train_runtime': 393.209, 'train_tokens_per_second': 754.042}\n",
      " 62% 120/192 [06:50<04:07,  3.44s/it][INFO|2025-06-08 18:19:00] llamafactory.train.callbacks:143 >> {'loss': 0.3397, 'learning_rate': 1.5812e-05, 'epoch': 1.89, 'throughput': 754.41}\n",
      "{'loss': 0.3397, 'grad_norm': 0.6407418251037598, 'learning_rate': 1.5812101585132417e-05, 'epoch': 1.89, 'num_input_tokens_seen': 309600, 'train_runtime': 410.3926, 'train_tokens_per_second': 754.4}\n",
      " 65% 125/192 [07:07<03:50,  3.43s/it][INFO|2025-06-08 18:19:17] llamafactory.train.callbacks:143 >> {'loss': 0.3243, 'learning_rate': 1.3943e-05, 'epoch': 1.97, 'throughput': 754.70}\n",
      "{'loss': 0.3243, 'grad_norm': 0.5646145939826965, 'learning_rate': 1.3942782744524973e-05, 'epoch': 1.97, 'num_input_tokens_seen': 322656, 'train_runtime': 427.532, 'train_tokens_per_second': 754.694}\n",
      " 68% 130/192 [07:21<03:05,  3.00s/it][INFO|2025-06-08 18:19:31] llamafactory.train.callbacks:143 >> {'loss': 0.2252, 'learning_rate': 1.2147e-05, 'epoch': 2.03, 'throughput': 755.22}\n",
      "{'loss': 0.2252, 'grad_norm': 0.5386804342269897, 'learning_rate': 1.2147431395169459e-05, 'epoch': 2.03, 'num_input_tokens_seen': 333568, 'train_runtime': 441.6879, 'train_tokens_per_second': 755.212}\n",
      " 70% 135/192 [07:38<03:10,  3.35s/it][INFO|2025-06-08 18:19:48] llamafactory.train.callbacks:143 >> {'loss': 0.2359, 'learning_rate': 1.0438e-05, 'epoch': 2.11, 'throughput': 755.56}\n",
      "{'loss': 0.2359, 'grad_norm': 0.6578263640403748, 'learning_rate': 1.043805757830495e-05, 'epoch': 2.11, 'num_input_tokens_seen': 346640, 'train_runtime': 458.7923, 'train_tokens_per_second': 755.549}\n",
      " 73% 140/192 [07:55<02:57,  3.41s/it][INFO|2025-06-08 18:20:05] llamafactory.train.callbacks:143 >> {'loss': 0.2001, 'learning_rate': 8.8261e-06, 'epoch': 2.19, 'throughput': 756.82}\n",
      "{'loss': 0.2001, 'grad_norm': 0.5321235656738281, 'learning_rate': 8.82609618662857e-06, 'epoch': 2.19, 'num_input_tokens_seen': 360160, 'train_runtime': 475.8907, 'train_tokens_per_second': 756.812}\n",
      " 76% 145/192 [08:13<02:40,  3.42s/it][INFO|2025-06-08 18:20:22] llamafactory.train.callbacks:143 >> {'loss': 0.2165, 'learning_rate': 7.3223e-06, 'epoch': 2.27, 'throughput': 756.57}\n",
      "{'loss': 0.2165, 'grad_norm': 0.4177156090736389, 'learning_rate': 7.3223304703363135e-06, 'epoch': 2.27, 'num_input_tokens_seen': 373008, 'train_runtime': 493.031, 'train_tokens_per_second': 756.561}\n",
      " 78% 150/192 [08:30<02:24,  3.45s/it][INFO|2025-06-08 18:20:40] llamafactory.train.callbacks:143 >> {'loss': 0.2008, 'learning_rate': 5.9368e-06, 'epoch': 2.35, 'throughput': 756.90}\n",
      "{'loss': 0.2008, 'grad_norm': 0.510010838508606, 'learning_rate': 5.936819902340299e-06, 'epoch': 2.35, 'num_input_tokens_seen': 386224, 'train_runtime': 510.2786, 'train_tokens_per_second': 756.888}\n",
      " 81% 155/192 [08:47<02:07,  3.45s/it][INFO|2025-06-08 18:20:57] llamafactory.train.callbacks:143 >> {'loss': 0.2024, 'learning_rate': 4.6788e-06, 'epoch': 2.43, 'throughput': 756.67}\n",
      "{'loss': 0.2024, 'grad_norm': 0.7806402444839478, 'learning_rate': 4.678832885209622e-06, 'epoch': 2.43, 'num_input_tokens_seen': 399168, 'train_runtime': 527.5365, 'train_tokens_per_second': 756.664}\n",
      " 83% 160/192 [09:04<01:50,  3.46s/it][INFO|2025-06-08 18:21:14] llamafactory.train.callbacks:143 >> {'loss': 0.1971, 'learning_rate': 3.5568e-06, 'epoch': 2.51, 'throughput': 756.74}\n",
      "{'loss': 0.1971, 'grad_norm': 0.32468825578689575, 'learning_rate': 3.5567847499932e-06, 'epoch': 2.51, 'num_input_tokens_seen': 412304, 'train_runtime': 544.8458, 'train_tokens_per_second': 756.735}\n",
      " 86% 165/192 [09:22<01:33,  3.45s/it][INFO|2025-06-08 18:21:31] llamafactory.train.callbacks:143 >> {'loss': 0.2054, 'learning_rate': 2.5782e-06, 'epoch': 2.59, 'throughput': 756.53}\n",
      "{'loss': 0.2054, 'grad_norm': 0.7191323041915894, 'learning_rate': 2.578181461682794e-06, 'epoch': 2.59, 'num_input_tokens_seen': 425216, 'train_runtime': 562.0669, 'train_tokens_per_second': 756.522}\n",
      " 89% 170/192 [09:39<01:15,  3.45s/it][INFO|2025-06-08 18:21:49] llamafactory.train.callbacks:143 >> {'loss': 0.2047, 'learning_rate': 1.7496e-06, 'epoch': 2.67, 'throughput': 756.27}\n",
      "{'loss': 0.2047, 'grad_norm': 0.5489206314086914, 'learning_rate': 1.7495694078996983e-06, 'epoch': 2.67, 'num_input_tokens_seen': 438096, 'train_runtime': 579.2879, 'train_tokens_per_second': 756.266}\n",
      " 91% 175/192 [09:56<00:58,  3.42s/it][INFO|2025-06-08 18:22:06] llamafactory.train.callbacks:143 >> {'loss': 0.2283, 'learning_rate': 1.0765e-06, 'epoch': 2.74, 'throughput': 756.29}\n",
      "{'loss': 0.2283, 'grad_norm': 0.5528128743171692, 'learning_rate': 1.0764916066947794e-06, 'epoch': 2.74, 'num_input_tokens_seen': 451056, 'train_runtime': 596.4081, 'train_tokens_per_second': 756.288}\n",
      " 94% 180/192 [10:13<00:41,  3.44s/it][INFO|2025-06-08 18:22:23] llamafactory.train.callbacks:143 >> {'loss': 0.2088, 'learning_rate': 5.6345e-07, 'epoch': 2.82, 'throughput': 756.16}\n",
      "{'loss': 0.2088, 'grad_norm': 0.5172816514968872, 'learning_rate': 5.634506264107053e-07, 'epoch': 2.82, 'num_input_tokens_seen': 463968, 'train_runtime': 613.5934, 'train_tokens_per_second': 756.149}\n",
      " 96% 185/192 [10:30<00:24,  3.44s/it][INFO|2025-06-08 18:22:40] llamafactory.train.callbacks:143 >> {'loss': 0.1972, 'learning_rate': 2.1388e-07, 'epoch': 2.90, 'throughput': 756.78}\n",
      "{'loss': 0.1972, 'grad_norm': 0.527222752571106, 'learning_rate': 2.1387846565474045e-07, 'epoch': 2.9, 'num_input_tokens_seen': 477376, 'train_runtime': 630.804, 'train_tokens_per_second': 756.774}\n",
      " 99% 190/192 [10:47<00:06,  3.42s/it][INFO|2025-06-08 18:22:57] llamafactory.train.callbacks:143 >> {'loss': 0.1757, 'learning_rate': 3.0114e-08, 'epoch': 2.98, 'throughput': 756.61}\n",
      "{'loss': 0.1757, 'grad_norm': 0.49419572949409485, 'learning_rate': 3.011359487068987e-08, 'epoch': 2.98, 'num_input_tokens_seen': 490224, 'train_runtime': 647.9309, 'train_tokens_per_second': 756.599}\n",
      "100% 192/192 [10:51<00:00,  2.53s/it][INFO|trainer.py:3993] 2025-06-08 18:23:01,723 >> Saving model checkpoint to saves/Qwen2.5-3B-Instruct/lora/train_2025-06-08-18-10-15/checkpoint-192\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 6d65950e-a538-40af-8a11-10c75863ff45)') - silently ignoring the lookup for the file config.json in Qwen/Qwen2.5-3B-Instruct.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen2.5-3B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2356] 2025-06-08 18:23:11,883 >> chat template saved in saves/Qwen2.5-3B-Instruct/lora/train_2025-06-08-18-10-15/checkpoint-192/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2525] 2025-06-08 18:23:11,884 >> tokenizer config file saved in saves/Qwen2.5-3B-Instruct/lora/train_2025-06-08-18-10-15/checkpoint-192/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2534] 2025-06-08 18:23:11,884 >> Special tokens file saved in saves/Qwen2.5-3B-Instruct/lora/train_2025-06-08-18-10-15/checkpoint-192/special_tokens_map.json\n",
      "[INFO|trainer.py:2676] 2025-06-08 18:23:12,337 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 662.4223, 'train_samples_per_second': 4.574, 'train_steps_per_second': 0.29, 'train_loss': 0.5609265969445308, 'epoch': 3.0, 'num_input_tokens_seen': 493104}\n",
      "100% 192/192 [11:02<00:00,  3.45s/it]\n",
      "[INFO|trainer.py:3993] 2025-06-08 18:23:12,339 >> Saving model checkpoint to saves/Qwen2.5-3B-Instruct/lora/train_2025-06-08-18-10-15\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: bf0dce2d-211b-4328-a5f0-7837928eebaf)') - silently ignoring the lookup for the file config.json in Qwen/Qwen2.5-3B-Instruct.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen2.5-3B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2356] 2025-06-08 18:23:22,494 >> chat template saved in saves/Qwen2.5-3B-Instruct/lora/train_2025-06-08-18-10-15/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2525] 2025-06-08 18:23:22,495 >> tokenizer config file saved in saves/Qwen2.5-3B-Instruct/lora/train_2025-06-08-18-10-15/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2534] 2025-06-08 18:23:22,495 >> Special tokens file saved in saves/Qwen2.5-3B-Instruct/lora/train_2025-06-08-18-10-15/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  num_input_tokens_seen    =     493104\n",
      "  total_flos               =  7686944GF\n",
      "  train_loss               =     0.5609\n",
      "  train_runtime            = 0:11:02.42\n",
      "  train_samples_per_second =      4.574\n",
      "  train_steps_per_second   =       0.29\n",
      "Figure saved at: saves/Qwen2.5-3B-Instruct/lora/train_2025-06-08-18-10-15/training_loss.png\n",
      "[WARNING|2025-06-08 18:23:22] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
      "[WARNING|2025-06-08 18:23:22] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
      "[INFO|modelcard.py:450] 2025-06-08 18:23:22,880 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:26:06,135 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:26:06,135 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:26:06,135 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:26:06,135 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:26:06,135 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:26:06,135 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:26:06,135 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2299] 2025-06-08 18:26:06,492 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:698] 2025-06-08 18:27:06,632 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-06-08 18:27:06,634 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 70,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:29:16,904 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:29:16,904 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:29:16,904 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:29:16,904 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:29:16,904 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:29:16,904 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:29:16,904 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2299] 2025-06-08 18:29:17,260 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:698] 2025-06-08 18:29:17,526 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-06-08 18:29:17,527 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 70,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|2025-06-08 18:29:17] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
      "[INFO|modeling_utils.py:1151] 2025-06-08 18:29:17,821 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:2241] 2025-06-08 18:29:17,822 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1135] 2025-06-08 18:29:17,825 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.04s/it]\n",
      "[INFO|modeling_utils.py:5131] 2025-06-08 18:29:20,113 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:5139] 2025-06-08 18:29:20,114 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-3B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1090] 2025-06-08 18:29:20,348 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/generation_config.json\n",
      "[INFO|configuration_utils.py:1135] 2025-06-08 18:29:20,348 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "[INFO|2025-06-08 18:29:20] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-06-08 18:29:21] llamafactory.model.adapter:143 >> Merged 1 adapter(s).\n",
      "[INFO|2025-06-08 18:29:21] llamafactory.model.adapter:143 >> Loaded adapter(s): saves/Qwen2.5-3B-Instruct/lora/train_2025-06-08-18-10-15\n",
      "[INFO|2025-06-08 18:29:21] llamafactory.model.loader:143 >> all params: 3,085,938,688\n",
      "[WARNING|2025-06-08 18:29:21] llamafactory.chat.hf_engine:154 >> There is no current event loop, creating a new one.\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:32:34,917 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:32:34,918 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:32:34,918 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:32:34,918 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:32:34,918 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:32:34,918 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:32:34,918 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2299] 2025-06-08 18:32:35,289 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:698] 2025-06-08 18:33:17,236 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-06-08 18:33:17,237 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 70,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:33:37,507 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:33:37,507 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:33:37,507 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:33:37,507 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:33:37,507 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:33:37,507 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-06-08 18:33:37,507 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2299] 2025-06-08 18:33:37,872 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:698] 2025-06-08 18:33:47,919 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-06-08 18:33:47,920 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 70,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|2025-06-08 18:33:47] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
      "[INFO|modeling_utils.py:1151] 2025-06-08 18:33:47,922 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:2241] 2025-06-08 18:33:47,923 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1135] 2025-06-08 18:33:47,924 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100% 2/2 [00:00<00:00, 18.69it/s]\n",
      "[INFO|modeling_utils.py:5131] 2025-06-08 18:33:48,081 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:5139] 2025-06-08 18:33:48,082 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-3B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1090] 2025-06-08 18:33:54,068 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/generation_config.json\n",
      "[INFO|configuration_utils.py:1135] 2025-06-08 18:33:54,069 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "[INFO|2025-06-08 18:33:59] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-06-08 18:34:10] llamafactory.model.adapter:143 >> Merged 1 adapter(s).\n",
      "[INFO|2025-06-08 18:34:10] llamafactory.model.adapter:143 >> Loaded adapter(s): saves/Qwen2.5-3B-Instruct/lora/train_2025-06-08-18-10-15\n",
      "[INFO|2025-06-08 18:34:10] llamafactory.model.loader:143 >> all params: 3,085,938,688\n",
      "[INFO|2025-06-08 18:34:10] llamafactory.train.tuner:143 >> Convert model dtype to: torch.bfloat16.\n",
      "[INFO|configuration_utils.py:424] 2025-06-08 18:34:10,145 >> Configuration saved in qwen2.5-3b-instruct-finetuning/config.json\n",
      "[INFO|configuration_utils.py:904] 2025-06-08 18:34:10,146 >> Configuration saved in qwen2.5-3b-instruct-finetuning/generation_config.json\n",
      "[INFO|modeling_utils.py:3733] 2025-06-08 18:34:29,197 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at qwen2.5-3b-instruct-finetuning/model.safetensors.index.json.\n",
      "[INFO|tokenization_utils_base.py:2356] 2025-06-08 18:34:29,198 >> chat template saved in qwen2.5-3b-instruct-finetuning/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2525] 2025-06-08 18:34:29,198 >> tokenizer config file saved in qwen2.5-3b-instruct-finetuning/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2534] 2025-06-08 18:34:29,198 >> Special tokens file saved in qwen2.5-3b-instruct-finetuning/special_tokens_map.json\n",
      "[INFO|2025-06-08 18:34:29] llamafactory.train.tuner:143 >> Ollama modelfile saved in qwen2.5-3b-instruct-finetuning/Modelfile\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 3075, in block_thread\n",
      "    time.sleep(0.1)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 151, in main\n",
      "    COMMAND_MAP[command]()\n",
      "  File \"/content/LLaMA-Factory/src/llamafactory/webui/interface.py\", line 97, in run_web_ui\n",
      "    create_ui().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2981, in launch\n",
      "    self.block_thread()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 3077, in block_thread\n",
      "    print(\"Keyboard interruption in main thread... closing server.\")\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/console_capture.py\", line 147, in write_with_callbacks\n",
      "    n = orig_write(s)\n",
      "        ^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Killing tunnel 0.0.0.0:7860 <> https://8f5f3dd56cb43d78f0.gradio.live\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "!GRADIO_SHARE=1 llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DX3j3MLrm46"
   },
   "source": [
    "# Una vez exportado el resultado del finetuning los subimos a hugginface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "6c1f510ef2494ec2ab61104c827387a6",
      "80833df1d4fd45fcaea3f16956e02e86",
      "edd005b434b740f5ba2f2fb80e72fd76",
      "5ac75dd572da4daaaf2c621bf9541046",
      "33140016df1c42d799afbc54a7ecadd9",
      "f25db810949145f2be3cdda4bbf0ba13",
      "1c08e0976926469aa5d159536ded08e8",
      "8dfcc3da0f964962be7473403b38b6e2",
      "c2c717114706435195c597716221ee05",
      "a25f5f3e1b714dea8d3cd1ed9ec8d937",
      "70660b3cb8e648ea8581c859c4dbc829",
      "206368febc5445178efe61461beb4316",
      "f48c3441079645459ae73f6b5d98acb0",
      "f0638f6dbe2540b3bb8f8cedc3444a75",
      "577762bf16ce4aa084ebfbc3a6d2899f",
      "027d1303d0ea48f69756f347cb8a3e14",
      "fc1becfeac424f3ab8fa7e05c19e67f9",
      "aaab7c43ed444903abb7510f339c410d",
      "d63c508690464c48a86824b4b641be02",
      "44698316283b43e5bb646abbd144c66a"
     ]
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1749408350347,
     "user": {
      "displayName": "Leonidas Moreno Vásquez",
      "userId": "09981213990272163392"
     },
     "user_tz": -120
    },
    "id": "kHAqicDN2F-K",
    "outputId": "9da101d8-e01e-4953-d286-5ff2d0658983"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1f510ef2494ec2ab61104c827387a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197,
     "referenced_widgets": [
      "467060a52ce7407cb907fffe1529b269",
      "76517010977c4c868fece96e94d58368",
      "09e76d864c714d3cba25cf6460d1e28a",
      "457d4031460741ae8ae9e90817412ca1",
      "b0b572196e5e4e55bef9df10cdda6f7f",
      "01bb46a43be9466085d4f9103b9113e9",
      "9d9c7d3286904851bd1c05467ad09136",
      "2080f340c65b46108b535a5eddc0e128",
      "2fec4620ddb64ab589106dac5750a4b3",
      "7b1ac27758a04d30892e3c59e87f89ab",
      "fa70e0080eba4a02aace9f3d2f68be16",
      "74036c2ff23848cf8d0b1688ad7cde7d",
      "73b08592fd994436bb3c32f1889c9635",
      "efef24b4fa8845ae86ba489342fa465e",
      "9ad550a50fe34b74a90d2f50a22a2e36",
      "ce031d427e224c1fbdb2be902b7cae54",
      "bc8ebf4a26fa45128d1b9a85d5cad874",
      "0583fcffd4dd44468d218d660e28ec2f",
      "5faf6082cb8243aab17542a523c7681e",
      "89ba8e374e2e4974932606de5e0abe7e",
      "bd6db82460144c97852ddb46b32cf9e3",
      "644aeebc0ab64992bd9131b77e4c290f",
      "9bb23b1286b3462cba2cb54b49f3581a",
      "9a5b8fe7b3d440e7960b8bcac7fd72d8",
      "f4350d1d3ab040ef837bf2e391e80741",
      "657d03d0a1104332b3ab7e41ac4f24a2",
      "a6c40a55ddeb4fcbac68aa653519dd37",
      "8d7c6882dc684513a266ac3934e436a2",
      "c6bdc2155612454cb5760cf740d8cf93",
      "e936a8ad21cb412e81f0673a03f1ceb7",
      "fb4ebde3175546ff969d4d7572b92aa6",
      "56264142b38447bab29f360c9ecc4869",
      "ce81da7774ac43dda85cbe97f09d609e",
      "0103fb4f2fc84b0fbc98b9b37ff9379b",
      "6984cfe432a849e9b488708157f6a201",
      "4cee30229913404baf22b5a95130325f",
      "182e75f28a7c4a38a3ded9d25e29fde0",
      "40382ff59fa54da6bf2f1f035d7a547c",
      "65cb8e8319694727a5af9407219f36ba",
      "afe849c6d79f4740a05d8dfed4e125b0",
      "0f90e113f22d44eeba922adde073d353",
      "57be3de5c4b747b6a08190e5ef2e46c4",
      "24dcb3363c7f473f83d1f9c551a48e34",
      "52a998f7d00f40a38060db8e60ae1faf"
     ]
    },
    "executionInfo": {
     "elapsed": 221687,
     "status": "ok",
     "timestamp": 1749409639702,
     "user": {
      "displayName": "Leonidas Moreno Vásquez",
      "userId": "09981213990272163392"
     },
     "user_tz": -120
    },
    "id": "COmCQ7dHKPYV",
    "outputId": "fe74182e-b5a0-4c24-f629-9d2a9b2987d6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467060a52ce7407cb907fffe1529b269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74036c2ff23848cf8d0b1688ad7cde7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb23b1286b3462cba2cb54b49f3581a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.21G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0103fb4f2fc84b0fbc98b9b37ff9379b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/leonidasmv/qwen2.5-3b-instruct-spanish-accent-finetuning/commit/17f3d88c680ad9e1b596c2f942ebeab99491a64b', commit_message='Upload folder using huggingface_hub', commit_description='', oid='17f3d88c680ad9e1b596c2f942ebeab99491a64b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/leonidasmv/qwen2.5-3b-instruct-spanish-accent-finetuning', endpoint='https://huggingface.co', repo_type='model', repo_id='leonidasmv/qwen2.5-3b-instruct-spanish-accent-finetuning'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo, upload_folder\n",
    "\n",
    "repo_id = \"leonidasmv/qwen2.5-3b-instruct-spanish-accent-finetuning\"\n",
    "create_repo(repo_id, exist_ok=True)\n",
    "\n",
    "upload_folder(\n",
    "    folder_path=\"/content/LLaMA-Factory/qwen2.5-3b-instruct-finetuning\",\n",
    "    repo_id=repo_id,\n",
    "    path_in_repo=\"\",\n",
    "    repo_type=\"model\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyND4XK59JYP8NK93swTfshA",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0103fb4f2fc84b0fbc98b9b37ff9379b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6984cfe432a849e9b488708157f6a201",
       "IPY_MODEL_4cee30229913404baf22b5a95130325f",
       "IPY_MODEL_182e75f28a7c4a38a3ded9d25e29fde0"
      ],
      "layout": "IPY_MODEL_40382ff59fa54da6bf2f1f035d7a547c"
     }
    },
    "01bb46a43be9466085d4f9103b9113e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "027d1303d0ea48f69756f347cb8a3e14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0583fcffd4dd44468d218d660e28ec2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "09e76d864c714d3cba25cf6460d1e28a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2080f340c65b46108b535a5eddc0e128",
      "max": 11421896,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2fec4620ddb64ab589106dac5750a4b3",
      "value": 11421896
     }
    },
    "0f90e113f22d44eeba922adde073d353": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "182e75f28a7c4a38a3ded9d25e29fde0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24dcb3363c7f473f83d1f9c551a48e34",
      "placeholder": "​",
      "style": "IPY_MODEL_52a998f7d00f40a38060db8e60ae1faf",
      "value": " 3/3 [03:19&lt;00:00, 199.26s/it]"
     }
    },
    "1c08e0976926469aa5d159536ded08e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "206368febc5445178efe61461beb4316": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2080f340c65b46108b535a5eddc0e128": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24dcb3363c7f473f83d1f9c551a48e34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fec4620ddb64ab589106dac5750a4b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "33140016df1c42d799afbc54a7ecadd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_f0638f6dbe2540b3bb8f8cedc3444a75",
      "style": "IPY_MODEL_577762bf16ce4aa084ebfbc3a6d2899f",
      "tooltip": ""
     }
    },
    "40382ff59fa54da6bf2f1f035d7a547c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44698316283b43e5bb646abbd144c66a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "457d4031460741ae8ae9e90817412ca1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b1ac27758a04d30892e3c59e87f89ab",
      "placeholder": "​",
      "style": "IPY_MODEL_fa70e0080eba4a02aace9f3d2f68be16",
      "value": " 11.4M/11.4M [00:02&lt;00:00, 8.73MB/s]"
     }
    },
    "467060a52ce7407cb907fffe1529b269": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_76517010977c4c868fece96e94d58368",
       "IPY_MODEL_09e76d864c714d3cba25cf6460d1e28a",
       "IPY_MODEL_457d4031460741ae8ae9e90817412ca1"
      ],
      "layout": "IPY_MODEL_b0b572196e5e4e55bef9df10cdda6f7f"
     }
    },
    "4cee30229913404baf22b5a95130325f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f90e113f22d44eeba922adde073d353",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_57be3de5c4b747b6a08190e5ef2e46c4",
      "value": 3
     }
    },
    "52a998f7d00f40a38060db8e60ae1faf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "56264142b38447bab29f360c9ecc4869": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "577762bf16ce4aa084ebfbc3a6d2899f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "57be3de5c4b747b6a08190e5ef2e46c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5ac75dd572da4daaaf2c621bf9541046": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_206368febc5445178efe61461beb4316",
      "style": "IPY_MODEL_f48c3441079645459ae73f6b5d98acb0",
      "value": true
     }
    },
    "5faf6082cb8243aab17542a523c7681e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "644aeebc0ab64992bd9131b77e4c290f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "657d03d0a1104332b3ab7e41ac4f24a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56264142b38447bab29f360c9ecc4869",
      "placeholder": "​",
      "style": "IPY_MODEL_ce81da7774ac43dda85cbe97f09d609e",
      "value": " 1.21G/1.21G [00:52&lt;00:00, 28.2MB/s]"
     }
    },
    "65cb8e8319694727a5af9407219f36ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6984cfe432a849e9b488708157f6a201": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65cb8e8319694727a5af9407219f36ba",
      "placeholder": "​",
      "style": "IPY_MODEL_afe849c6d79f4740a05d8dfed4e125b0",
      "value": "Upload 3 LFS files: 100%"
     }
    },
    "6c1f510ef2494ec2ab61104c827387a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [],
      "layout": "IPY_MODEL_1c08e0976926469aa5d159536ded08e8"
     }
    },
    "70660b3cb8e648ea8581c859c4dbc829": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73b08592fd994436bb3c32f1889c9635": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc8ebf4a26fa45128d1b9a85d5cad874",
      "placeholder": "​",
      "style": "IPY_MODEL_0583fcffd4dd44468d218d660e28ec2f",
      "value": "model-00001-of-00002.safetensors: 100%"
     }
    },
    "74036c2ff23848cf8d0b1688ad7cde7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_73b08592fd994436bb3c32f1889c9635",
       "IPY_MODEL_efef24b4fa8845ae86ba489342fa465e",
       "IPY_MODEL_9ad550a50fe34b74a90d2f50a22a2e36"
      ],
      "layout": "IPY_MODEL_ce031d427e224c1fbdb2be902b7cae54"
     }
    },
    "76517010977c4c868fece96e94d58368": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_01bb46a43be9466085d4f9103b9113e9",
      "placeholder": "​",
      "style": "IPY_MODEL_9d9c7d3286904851bd1c05467ad09136",
      "value": "tokenizer.json: 100%"
     }
    },
    "7b1ac27758a04d30892e3c59e87f89ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80833df1d4fd45fcaea3f16956e02e86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8dfcc3da0f964962be7473403b38b6e2",
      "placeholder": "​",
      "style": "IPY_MODEL_c2c717114706435195c597716221ee05",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "89ba8e374e2e4974932606de5e0abe7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8d7c6882dc684513a266ac3934e436a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8dfcc3da0f964962be7473403b38b6e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a5b8fe7b3d440e7960b8bcac7fd72d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d7c6882dc684513a266ac3934e436a2",
      "placeholder": "​",
      "style": "IPY_MODEL_c6bdc2155612454cb5760cf740d8cf93",
      "value": "model-00002-of-00002.safetensors: 100%"
     }
    },
    "9ad550a50fe34b74a90d2f50a22a2e36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd6db82460144c97852ddb46b32cf9e3",
      "placeholder": "​",
      "style": "IPY_MODEL_644aeebc0ab64992bd9131b77e4c290f",
      "value": " 4.96G/4.96G [03:18&lt;00:00, 28.2MB/s]"
     }
    },
    "9bb23b1286b3462cba2cb54b49f3581a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9a5b8fe7b3d440e7960b8bcac7fd72d8",
       "IPY_MODEL_f4350d1d3ab040ef837bf2e391e80741",
       "IPY_MODEL_657d03d0a1104332b3ab7e41ac4f24a2"
      ],
      "layout": "IPY_MODEL_a6c40a55ddeb4fcbac68aa653519dd37"
     }
    },
    "9d9c7d3286904851bd1c05467ad09136": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a25f5f3e1b714dea8d3cd1ed9ec8d937": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6c40a55ddeb4fcbac68aa653519dd37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aaab7c43ed444903abb7510f339c410d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d63c508690464c48a86824b4b641be02",
      "placeholder": "​",
      "style": "IPY_MODEL_44698316283b43e5bb646abbd144c66a",
      "value": "Connecting..."
     }
    },
    "afe849c6d79f4740a05d8dfed4e125b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0b572196e5e4e55bef9df10cdda6f7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc8ebf4a26fa45128d1b9a85d5cad874": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd6db82460144c97852ddb46b32cf9e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2c717114706435195c597716221ee05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6bdc2155612454cb5760cf740d8cf93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ce031d427e224c1fbdb2be902b7cae54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce81da7774ac43dda85cbe97f09d609e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d63c508690464c48a86824b4b641be02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e936a8ad21cb412e81f0673a03f1ceb7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "edd005b434b740f5ba2f2fb80e72fd76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_a25f5f3e1b714dea8d3cd1ed9ec8d937",
      "placeholder": "​",
      "style": "IPY_MODEL_70660b3cb8e648ea8581c859c4dbc829",
      "value": ""
     }
    },
    "efef24b4fa8845ae86ba489342fa465e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5faf6082cb8243aab17542a523c7681e",
      "max": 4957560304,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_89ba8e374e2e4974932606de5e0abe7e",
      "value": 4957560304
     }
    },
    "f0638f6dbe2540b3bb8f8cedc3444a75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f25db810949145f2be3cdda4bbf0ba13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_027d1303d0ea48f69756f347cb8a3e14",
      "placeholder": "​",
      "style": "IPY_MODEL_fc1becfeac424f3ab8fa7e05c19e67f9",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "f4350d1d3ab040ef837bf2e391e80741": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e936a8ad21cb412e81f0673a03f1ceb7",
      "max": 1214366696,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fb4ebde3175546ff969d4d7572b92aa6",
      "value": 1214366696
     }
    },
    "f48c3441079645459ae73f6b5d98acb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa70e0080eba4a02aace9f3d2f68be16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb4ebde3175546ff969d4d7572b92aa6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fc1becfeac424f3ab8fa7e05c19e67f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
